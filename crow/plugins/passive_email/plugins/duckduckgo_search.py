# crow/plugins/passive_email/plugins/duckduckgo_search.py
from __future__ import annotations

import random
import re
import time
from typing import Dict, List, Set
from urllib.parse import quote_plus, urlparse

import base
import requests


class Plugin:
    """
    Engine name: duckduckgo
    Searches DuckDuckGo for emails related to a domain.
    """

    def __init__(self, harvester, opts: Dict):
        self.harvester = harvester
        self.opts = opts or {}
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        logger.info(
            f"DuckDuckGo plugin initialized with opts: {list(self.opts.keys())}"
        )
        self.harvester.register_plugin("duckduckgo", {"search": self.search})

    def search(self, domain: str, limit: int = 100) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ DuckDuckGo Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚"""
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¨ÙˆØ¶ÙˆØ­
        logger.info(f"ğŸš€ DUCKDUCKGO SEARCH STARTING for domain: {domain}")
        logger.info(f"DuckDuckGo proxy setting: {self.opts.get('proxy', 'No proxy')}")

        domain = base.normalize_domain(domain)

        max_pages = int(self.opts.get("ddg_pages", 2))
        base_delay = float(self.opts.get("ddg_delay", 2.0))
        max_results = min(limit, max_pages * 30)

        user_agent = self._get_user_agent()
        proxy = self._get_proxy()

        queries = self._generate_queries(domain)

        all_emails: Set[str] = set()
        session = requests.Session()

        logger.info(f"DuckDuckGo: Using {len(queries)} queries, {max_pages} pages each")
        logger.info(f"DuckDuckGo queries: {queries}")

        for query_idx, query in enumerate(queries):
            if len(all_emails) >= max_results:
                break

            logger.info(
                f"DuckDuckGo processing query {query_idx + 1}/{len(queries)}: '{query}'"
            )

            for page in range(max_pages):
                if len(all_emails) >= max_results:
                    break

                try:
                    logger.info(
                        f"DuckDuckGo search: '{query}' - Page {page + 1}/{max_pages}"
                    )
                    emails = self._search_ddg_page(
                        session=session,
                        query=query,
                        page=page,
                        user_agent=user_agent,
                        proxy=proxy,
                    )

                    new_emails = [e for e in emails if e not in all_emails]
                    if new_emails:
                        logger.info(
                            f"DuckDuckGo: Found {len(new_emails)} new emails from query '{query}'"
                        )
                        for email in new_emails[:3]:  # ØªØ³Ø¬ÙŠÙ„ Ø£ÙˆÙ„ 3 Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª ÙÙ‚Ø·
                            logger.debug(f"DuckDuckGo email found: {email}")

                    all_emails.update(emails)

                    # ØªØ£Ø®ÙŠØ± Ø°ÙƒÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
                    delay = random.uniform(base_delay, base_delay + 1.5)
                    logger.debug(f"DuckDuckGo: Sleeping for {delay:.1f} seconds")
                    time.sleep(delay)

                except Exception as e:
                    logger.error(
                        f"DuckDuckGo search error for query '{query}': {str(e)}"
                    )
                    logger.exception("DuckDuckGo error details:")
                    continue

        # ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        filtered = base.filter_by_domain(list(all_emails), domain)

        logger.info(
            f"âœ… DUCKDUCKGO SEARCH COMPLETED. Found {len(filtered)} emails for {domain}"
        )
        if filtered:
            logger.info(f"DuckDuckGo emails found: {filtered[:5]}")  # Ø£ÙˆÙ„ 5 Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª ÙÙ‚Ø·

        return filtered[:limit]

    def _generate_queries(self, domain: str) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø­Ø« Ù„Ù€ DuckDuckGo"""
        queries = [
            f'site:{domain} "@{domain}"',
            f'site:{domain} "email"',
            f'site:{domain} "contact"',
            f'"{domain}" "@gmail.com"',
            f'"{domain}" "@yahoo.com"',
            f'"{domain}" "@hotmail.com"',
            f'"{domain}" "@outlook.com"',
        ]

        # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ø·Ø§Ù‚ .ye (Ù…Ø´Ø§ÙƒÙ„ Ù…Ø­ØªÙ…Ù„Ø©)
        if domain.endswith(".ye"):
            logger.warning(f"Domain {domain} ends with .ye - using limited queries")
            queries = queries[:3]  # ÙÙ‚Ø· Ø£ÙˆÙ„ 3 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª

        return queries

    def _search_ddg_page(
        self, session, query: str, page: int, user_agent: str, proxy: str = None
    ) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ØµÙØ­Ø© Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† DuckDuckGo"""
        s = page * 30
        encoded_query = quote_plus(query)

        # âœ… **ØªÙ… Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ù†Ø§** - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ØµØ­ÙŠØ­
        url = f"https://duckduckgo.com/html/?q={encoded_query}&s={s}"

        headers = self._create_headers(user_agent)
        proxies = self._create_proxies(proxy)

        logger.info(f"DuckDuckGo requesting: {url}")
        logger.debug(f"DuckDuckGo headers: {headers}")

        try:
            # âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©: DuckDuckGo ÙŠØªÙˆÙ‚Ø¹ GET ÙˆÙ„ÙŠØ³ POST
            response = session.get(
                url, headers=headers, proxies=proxies, timeout=15, allow_redirects=True
            )

            logger.info(f"DuckDuckGo response status: {response.status_code}")

            if response.status_code == 200:
                emails = base.extract_emails(response.text)
                logger.debug(f"DuckDuckGo raw emails found: {len(emails)}")

                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                cleaned_emails = []
                for email in emails:
                    email = email.strip().lower()
                    if self._is_valid_email(email):
                        cleaned_emails.append(email)

                logger.info(
                    f"âœ… DuckDuckGo found {len(cleaned_emails)} valid emails on page {page}"
                )
                return cleaned_emails
            else:
                logger.warning(
                    f"âš ï¸ DuckDuckGo: HTTP {response.status_code} for query: {query}"
                )

        except requests.exceptions.Timeout:
            logger.warning("â° DuckDuckGo: Timeout for query: {query}")
        except requests.exceptions.ConnectionError as e:
            logger.error(f"ğŸ”Œ DuckDuckGo connection error: {str(e)}")
        except Exception as e:
            logger.error(f"âŒ DuckDuckGo request error: {str(e)}")
            logger.exception("DuckDuckGo exception details:")

        return []

    def _create_headers(self, user_agent: str) -> Dict[str, str]:
        """Ø¥Ù†Ø´Ø§Ø¡ headers Ù„Ù€ DuckDuckGo"""
        return {
            "User-Agent": user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Cache-Control": "max-age=0",
        }

    def _create_proxies(self, proxy):
        """Ø¥Ù†Ø´Ø§Ø¡ proxies dictionary"""
        if not proxy:
            logger.debug("DuckDuckGo: No proxy configured")
            return None

        try:
            if isinstance(proxy, str):
                parsed = urlparse(proxy)
            else:
                parsed = proxy

            if parsed.scheme and parsed.netloc:
                scheme = parsed.scheme
                proxy_url = f"{scheme}://{parsed.netloc}"
                logger.info(f"DuckDuckGo using proxy: {proxy_url}")
                return {"http": proxy_url, "https": proxy_url}
        except Exception as e:
            logger.error(f"Error parsing proxy: {str(e)}")

        return None

    def _get_user_agent(self) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ User-Agent"""
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

        ua = self.opts.get("useragent") or self.opts.get("userAgent")
        selected_ua = ua or random.choice(user_agents)
        logger.debug(f"DuckDuckGo User-Agent: {selected_ua[:50]}...")
        return selected_ua

    def _get_proxy(self):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù€ proxy"""
        proxy = self.opts.get("proxy")
        if not proxy:
            logger.debug("DuckDuckGo: No proxy configured")
            return None

        if hasattr(proxy, "scheme") and hasattr(proxy, "netloc"):
            logger.debug(f"DuckDuckGo: Using proxy {proxy.scheme}://{proxy.netloc}")
            return proxy

        if isinstance(proxy, str):
            try:
                parsed = urlparse(proxy)
                logger.debug(
                    f"DuckDuckGo: Parsed proxy string: {parsed.scheme}://{parsed.netloc}"
                )
                return parsed
            except Exception as e:
                logger.error(f"DuckDuckGo: Error parsing proxy string: {str(e)}")
                return None

        return None

    def _is_valid_email(self, email: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„"""
        if not email or "@" not in email:
            return False

        # ØªØ¬Ø§Ù‡Ù„ Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„ÙˆÙ‡Ù…ÙŠ
        disposable_domains = {
            "mailinator.com",
            "guerrillamail.com",
            "10minutemail.com",
            "tempmail.com",
            "yopmail.com",
            "trashmail.com",
            "temp-mail.org",
            "fakeinbox.com",
            "getairmail.com",
        }

        domain = email.split("@")[-1].lower()
        if domain in disposable_domains:
            return False

        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² ØºÙŠØ± Ø¹Ø§Ø¯ÙŠØ©
        if "..." in email or ".." in email:
            return False

        # ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙŠØºØ© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        email_regex = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
        return bool(re.match(email_regex, email))


# ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù€ logger - ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù‡Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù„Ù
try:
    from crow.core.logger import logger
except ImportError:
    import logging

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    # Ø¥Ø¶Ø§ÙØ© handler Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
