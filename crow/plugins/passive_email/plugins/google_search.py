# crow/plugins/passive_email/plugins/google_search.py
from __future__ import annotations

import random
import re
import time
from typing import Dict, List, Set
from urllib.parse import quote_plus, urlparse

import base
import requests


class Plugin:
    """
    Engine name: google
    Searches Google for emails related to a domain.
    Opts supported:
      - google_pages: number of pages to scrape (default: 2)
      - google_delay: delay between requests in seconds (default: 3-6)
    """

    def __init__(self, harvester, opts: Dict):
        self.harvester = harvester
        self.opts = opts or {}
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        logger.info(f"Google plugin initialized with opts: {list(self.opts.keys())}")
        self.harvester.register_plugin("google", {"search": self.search})

    def search(self, domain: str, limit: int = 100) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚"""
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¨ÙˆØ¶ÙˆØ­
        logger.info(f"ğŸš€ GOOGLE SEARCH STARTING for domain: {domain}")
        logger.info(
            f"Google search options: pages={self.opts.get('google_pages', 2)}, "
            f"delay={self.opts.get('google_delay', 3.0)}"
        )
        logger.info(f"Google proxy setting: {self.opts.get('proxy', 'No proxy')}")

        domain = base.normalize_domain(domain)

        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ®ØµÙŠØµ
        max_pages = int(self.opts.get("google_pages", 2))
        base_delay = float(self.opts.get("google_delay", 3.0))
        max_results = min(limit, max_pages * 10)

        user_agent = self._get_user_agent()
        proxy = self._get_proxy()

        # Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø­Ø« Ø°ÙƒÙŠØ© (Ù…Ø­Ø¯ÙˆØ¯Ø©)
        queries = self._generate_queries(domain)

        all_emails: Set[str] = set()
        session = requests.Session()

        logger.info(f"Google: Using {len(queries)} queries, {max_pages} pages each")
        logger.info(f"Google queries: {queries}")

        for query_idx, query in enumerate(queries):
            if len(all_emails) >= max_results:
                break

            logger.info(
                f"Google processing query {query_idx + 1}/{len(queries)}: '{query}'"
            )

            for page in range(max_pages):
                if len(all_emails) >= max_results:
                    break

                try:
                    logger.info(
                        f"Google search: '{query}' - Page {page + 1}/{max_pages}"
                    )
                    emails = self._search_google_page(
                        session=session,
                        query=query,
                        page=page,
                        user_agent=user_agent,
                        proxy=proxy,
                    )

                    new_emails = [e for e in emails if e not in all_emails]
                    if new_emails:
                        logger.info(
                            f"Google: Found {len(new_emails)} new emails from query '{query}'"
                        )
                        for email in new_emails[:3]:  # ØªØ³Ø¬ÙŠÙ„ Ø£ÙˆÙ„ 3 Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª ÙÙ‚Ø·
                            logger.debug(f"Google email found: {email}")

                    all_emails.update(emails)

                    # ØªØ£Ø®ÙŠØ± Ø°ÙƒÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª (Ø£Ø·ÙˆÙ„ Ù„Ù€ Google)
                    delay = random.uniform(base_delay, base_delay + 3.0)
                    logger.debug(f"Google: Sleeping for {delay:.1f} seconds")
                    time.sleep(delay)

                except requests.exceptions.TooManyRedirects:
                    logger.warning("Google: Too many redirects, moving to next query")
                    break
                except requests.exceptions.ProxyError as e:
                    logger.error(f"Google: Proxy error: {str(e)}")
                    break
                except requests.exceptions.Timeout:
                    logger.warning("Google: Request timeout, continuing...")
                    time.sleep(5)
                    continue
                except Exception as e:
                    logger.error(f"Google search error: {str(e)}")
                    logger.exception("Google error details:")  # ØªØ³Ø¬ÙŠÙ„ traceback ÙƒØ§Ù…Ù„
                    continue

        # ØªØµÙÙŠØ© ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        filtered = base.filter_by_domain(list(all_emails), domain)
        sorted_emails = self._sort_emails_by_quality(filtered)

        logger.info(
            f"âœ… GOOGLE SEARCH COMPLETED. Found {len(sorted_emails)} emails for {domain}"
        )
        if sorted_emails:
            logger.info(
                f"Google emails found: {sorted_emails[:5]}"
            )  # Ø£ÙˆÙ„ 5 Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª ÙÙ‚Ø·

        return sorted_emails[:limit]

    def _generate_queries(self, domain: str) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø­Ø« Ø°ÙƒÙŠØ©"""
        # Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© (Ù…Ø­Ø¯ÙˆØ¯Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±)
        queries = [
            f'site:{domain} "@{domain}" email',
            f'site:{domain} "contact"',
            f'"{domain}" "@gmail.com" OR "@yahoo.com"',
        ]

        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ø·Ø§Ù‚ .ye (Ù…Ø´Ø§ÙƒÙ„ Ù…Ø­ØªÙ…Ù„Ø© Ù…Ø¹ Google)
        if domain.endswith(".ye"):
            logger.warning(f"Domain {domain} ends with .ye - using limited queries")
            queries = [f'site:{domain} "contact"']  # Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·

        return queries

    def _search_google_page(
        self, session, query: str, page: int, user_agent: str, proxy: str = None
    ) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ØµÙØ­Ø© Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† Google"""
        start = page * 10
        encoded_query = quote_plus(query)
        url = f"https://www.google.com/search?q={encoded_query}&start={start}&num=10"

        headers = self._create_headers(user_agent)
        proxies = self._create_proxies(proxy)

        logger.info(f"Google requesting: {url}")
        logger.debug(f"Google headers: {headers}")

        try:
            response = session.get(
                url,
                headers=headers,
                proxies=proxies,
                timeout=20,  # Ø²ÙŠØ§Ø¯Ø© timeout
                allow_redirects=True,
            )

            logger.info(f"Google response status: {response.status_code}")

            if response.status_code == 200:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù†Ù†Ø§ Ù„Ù… Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ ØµÙØ­Ø© Ø­Ø¸Ø±
                response_text = response.text.lower()

                if "detected unusual traffic" in response_text:
                    logger.warning(
                        "âš ï¸ Google has detected unusual traffic. Consider using proxies or increasing delay."
                    )
                    return []

                if "captcha" in response_text:
                    logger.warning(
                        "âš ï¸ Google CAPTCHA triggered. Increasing delay or using proxies recommended."
                    )
                    return []

                if "our systems have detected unusual traffic" in response_text:
                    logger.warning("âš ï¸ Google: Rate limit detected. Waiting longer...")
                    time.sleep(10)
                    return []

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª
                emails = base.extract_emails(response.text)
                logger.debug(f"Google raw emails found: {len(emails)}")

                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                cleaned_emails = []
                for email in emails:
                    email = email.strip().lower()
                    if self._is_valid_email(email):
                        cleaned_emails.append(email)

                logger.info(
                    f"âœ… Google found {len(cleaned_emails)} valid emails on page {page}"
                )
                return cleaned_emails

            elif response.status_code == 429:
                logger.warning(
                    "â¸ï¸ Google: Rate limited (429). Increasing delay to 15 seconds..."
                )
                time.sleep(15)
            elif response.status_code == 503:
                logger.warning(
                    "â¸ï¸ Google: Service unavailable (503). Possible blocking."
                )
                time.sleep(10)
            else:
                logger.warning(f"âš ï¸ Google: HTTP {response.status_code}")

        except requests.exceptions.ConnectionError as e:
            logger.error(f"ğŸ”Œ Google connection error: {str(e)}")
        except requests.exceptions.Timeout:
            logger.warning("â° Google: Request timeout (20s)")
        except Exception as e:
            logger.error(f"âŒ Google request error: {str(e)}")
            logger.exception("Google exception details:")

        return []

    def _create_headers(self, user_agent: str) -> Dict[str, str]:
        """Ø¥Ù†Ø´Ø§Ø¡ headers ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        return {
            "User-Agent": user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0",
        }

    def _create_proxies(self, proxy):
        """Ø¥Ù†Ø´Ø§Ø¡ proxies dictionary"""
        if not proxy:
            logger.debug("Google: No proxy configured")
            return None

        try:
            if isinstance(proxy, str):
                parsed = urlparse(proxy)
            else:
                parsed = proxy

            if parsed.scheme and parsed.netloc:
                scheme = parsed.scheme
                proxy_url = f"{scheme}://{parsed.netloc}"
                logger.info(f"Google using proxy: {proxy_url}")
                return {"http": proxy_url, "https": proxy_url}
        except Exception as e:
            logger.error(f"Error creating proxies: {str(e)}")

        return None

    def _get_user_agent(self) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ User-Agent Ø¹Ø´ÙˆØ§Ø¦ÙŠ"""
        user_agents = [
            # Chrome Ø¹Ù„Ù‰ Windows
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            # Firefox Ø¹Ù„Ù‰ Windows
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            # Safari Ø¹Ù„Ù‰ Mac
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
            # Chrome Ø¹Ù„Ù‰ Linux
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

        ua = self.opts.get("useragent") or self.opts.get("userAgent")
        selected_ua = ua or random.choice(user_agents)
        logger.debug(f"Google User-Agent: {selected_ua[:50]}...")
        return selected_ua

    def _get_proxy(self):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù€ proxy"""
        proxy = self.opts.get("proxy")
        if not proxy:
            logger.debug("Google: No proxy configured")
            return None

        if hasattr(proxy, "scheme") and hasattr(proxy, "netloc"):
            logger.debug(f"Google: Using proxy {proxy.scheme}://{proxy.netloc}")
            return proxy

        if isinstance(proxy, str):
            try:
                parsed = urlparse(proxy)
                logger.debug(
                    f"Google: Parsed proxy string: {parsed.scheme}://{parsed.netloc}"
                )
                return parsed
            except Exception as e:
                logger.error(f"Google: Error parsing proxy string: {str(e)}")
                return None

        return None

    def _is_valid_email(self, email: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„"""
        if not email or "@" not in email:
            return False

        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØ§Ø¶Ø­Ø© ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        unwanted_patterns = [
            r"\.png$",
            r"\.jpg$",
            r"\.gif$",  # Ù…Ù„ÙØ§Øª ØµÙˆØ±
            r"example\.com$",
            r"test\.com$",  # Ù†Ø·Ø§Ù‚Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            r"noreply@",
            r"no-reply@",  # Ø¨Ø±ÙŠØ¯ Ø¢Ù„ÙŠ
            r"bounce@",
            r"admin@server\.",  # Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø®Ø§Ø¯Ù…
            r"\.\.",
            r"\s",  # Ù†Ù‚Ø·ØªÙŠÙ† Ù…ØªØªØ§Ù„ÙŠØªÙŠÙ† Ø£Ùˆ Ù…Ø³Ø§ÙØ§Øª
        ]

        for pattern in unwanted_patterns:
            if re.search(pattern, email, re.IGNORECASE):
                return False

        # ØªØ¬Ø§Ù‡Ù„ Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„ÙˆÙ‡Ù…ÙŠ
        disposable_domains = {
            "mailinator.com",
            "guerrillamail.com",
            "10minutemail.com",
            "tempmail.com",
            "yopmail.com",
            "trashmail.com",
            "temp-mail.org",
            "fakeinbox.com",
            "getairmail.com",
            "sharklasers.com",
            "grr.la",
            "maildrop.cc",
        }

        domain = email.split("@")[-1].lower()
        if domain in disposable_domains:
            return False

        # ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙŠØºØ© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        email_regex = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
        return bool(re.match(email_regex, email))

    def _sort_emails_by_quality(self, emails: List[str]) -> List[str]:
        """ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø¬ÙˆØ¯Ø©"""

        def email_score(email: str) -> int:
            score = 0

            # Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ø£ÙØ¶Ù„
            if re.match(r"^[a-z]+\.[a-z]+@", email):  # john.doe@
                score += 3
            elif re.match(r"^[a-z]+@", email):  # john@
                score += 2

            # Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø£ÙØ¶Ù„
            corporate_domains = {"gmail.com", "yahoo.com", "hotmail.com", "outlook.com"}
            domain = email.split("@")[-1]
            if domain not in corporate_domains:
                score += 5  # Ù†Ø·Ø§Ù‚ Ù…Ø®ØµØµ Ù„Ù„Ø´Ø±ÙƒØ©

            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„
            if not re.match(r"^\d+", email.split("@")[0]):
                score += 1

            return score

        return sorted(emails, key=email_score, reverse=True)


# ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù€ logger - ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù‡Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù„Ù
try:
    from crow.core.logger import logger
except ImportError:
    import logging

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    # Ø¥Ø¶Ø§ÙØ© handler Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
